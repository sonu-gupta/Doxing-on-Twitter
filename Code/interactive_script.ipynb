{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('modeldoxing_distilBert/model', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(tweet):\n",
    "    tweet = tweet.replace('\\n',' ')\n",
    "    tweet = tweet.lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_check_result(test_encoding) :\n",
    "    input_ids = torch.tensor(test_encoding[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(test_encoding[\"attention_mask\"]).to(device)\n",
    "    with torch.no_grad() :\n",
    "        outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "        y = np.argmax(outputs[0].to('cpu').numpy())\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_pattern = re.compile(r'(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})')\n",
    "url_pattern = re.compile(\"((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anon_ip(twt):\n",
    "    if ip_pattern.search(twt) != None:\n",
    "        ips = re.findall(ip_pattern, twt)\n",
    "        for ip in ips:\n",
    "            twt = twt.replace(ip, 5*'X')\n",
    "        return twt\n",
    "    else:\n",
    "        return twt\n",
    "\n",
    "def anon_url(twt):\n",
    "    if url_pattern.search(twt) != None:\n",
    "        links = re.findall(url_pt, inp)\n",
    "        for lnk in links:\n",
    "            twt = twt.replace(lnk[0], 5*'X')\n",
    "            \n",
    "        return twt\n",
    "    else:\n",
    "        return twt\n",
    "    \n",
    "def generic_ner(twt):\n",
    "    doc = nlp(twt)\n",
    "    for ent in doc.ents:\n",
    "        twt = twt[0:ent.start_char] + 5 * 'X'+  twt[ent.end_char:]\n",
    "    return twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(org_twt):\n",
    "    if ip_pattern.search(org_twt) == None and ip_pattern.search(org_twt) == None:\n",
    "        return org_twt\n",
    "    else:\n",
    "        stp1 = anon_ip(org_twt)\n",
    "        stp2 = anon_url(stp1)\n",
    "        sanitized_twt = generic_ner(stp2)\n",
    "        \n",
    "        return sanitized_twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doxing_test(twt):\n",
    "    twt = pre_process(twt)\n",
    "    test_encoding = tokenizer(twt, truncation=True, padding=True)\n",
    "    input_ids = torch.tensor(test_encoding[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(test_encoding[\"attention_mask\"]).to(device)\n",
    "    op = to_check_result(test_encoding)        \n",
    "    return op\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's happening?....... \n",
      "Everyone can reply.\n",
      "Enter your tweet:\n",
      "@wife34_ Helllo. \\nFull name: Martha Benjamin\\nAddress: 14 Pug Street, Klon, US\\nIP address: 12.18.0.16\\nCordinates: 11.5033\\u00b0 N, 0.1096\\u00b0 W\n",
      "\n",
      "\n",
      "It's recommended to not post this tweet. It might be against Twitter's Policy.\n",
      "Twitter’s private information and media policy - “You may not publish or post other people's private information (such as home phone number and address) without their express authorization and \\permission. We also prohibit threatening to expose private information or incentivizing others to do so.”\n",
      "For more information, please visit https://help.twitter.com/en/rules-and-policies/personal-information\n",
      "Do you want to continue posting? Y/Ny\n",
      "\n",
      "\n",
      "@wife34_ Helllo. \\nFull name: XXXXX Benjamin\\nAddress: 1XXXXXXXXet, KloXXXXXSXXXXXddress: XXXXX\\nCordinates: 11.XXXXX.1096\\u00b0 XXXXX\n"
     ]
    }
   ],
   "source": [
    "input_ = str(input(\"What's happening?....... \\nEveryone can reply.\\nEnter your tweet:\\n\"))\n",
    "op = doxing_test(input_)\n",
    "if op == 0:\n",
    "    print('\\n')\n",
    "    print(\"It's not a doxed tweet\")\n",
    "else:\n",
    "    print('\\n')\n",
    "    print(\"It's recommended to not post this tweet. It might be against Twitter's Policy.\")\n",
    "    print(\"Twitter’s private information and media policy - “You may not publish or post other people's private information (such as home phone number and address) without their express authorization and \\permission. We also prohibit threatening to expose private information or incentivizing others to do so.”\")\n",
    "    print(\"For more information, please visit https://help.twitter.com/en/rules-and-policies/personal-information\")\n",
    "if op == 1:\n",
    "    nudge = str(input(\"Do you want to continue posting? Y/N\"))\n",
    "    if (nudge == 'N') or (nudge == 'n'):\n",
    "        print(\"\\nThe draft has been discarded.\")\n",
    "    elif (nudge == 'Y') or (nudge == 'y'):\n",
    "        print('\\n')\n",
    "        print(anonymize(input_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_doxed_tweet: [not from our dataset; created for this demo]\n",
    "    \n",
    "\n",
    "@wife34_ Helllo. \\nFull name: Martha Benjamin\\nAddress: 14 Pug Street, Klon, US\\nIP address: 12.18.0.16\\nCordinates: 11.5033\\u00b0 N, 0.1096\\u00b0 W\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyto",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
